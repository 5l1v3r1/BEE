\section{Introduction}
Modern scientific simulation and big data analytics workflows require sharing large amount of data between tasks. The data are usually stored in filesystems by producer tasks and later read out by consumer tasks. However, with increasing demands of solving more complicated problems and processing larger amounts of input data, sharing data between tasks via filesystems is inefficient. For example, simulation-analysis based workflow is one commonly used workflow in scientific computing. It basically comprises two parts: (1) simulation tasks, that are responsible for running simulations and dumping simulation output data and (2) analysis tasks, that are responsible for loading and analyzing simulation data, then present results to end users (e.g., statistic results, visualization images, etc.). In traditional simulation workflows (also known as pure offline dependency workflows \cite{sewell2015large}), analysis tasks need to wait for all simulation tasks to complete dumping all data to filesystems before processing data. However, as simulation problems become more complicated, the simulation data can reach hundreds of terabytes to petabytes. Storing and loading such large amounts of data on disks can significantly degrade overall performance of user's workflows and cause heavy burdens on filesystems. Recently, in situ analysis \cite{sewell2015large} was proposed to allow analysis tasks to run with simulation tasks side by side. It offers two benefits over traditional offline workflow: First, as simulations progress, they can send the simulation data to analysis tasks for real-time processing and output results simultaneously. The data can be transferred via either small-sized temporary or permanent data files on shared file systems or network communication between processes, which can greatly mitigate the heavy burdens on filesystems. Second, in situ workflow enables users to identify potential problems as the current simulations making processes, so that users can reconfigure, adjust, and restart simulations immediately to save time, get flexible real-time simulation control, or filter out less relevant data as needed to save computing resources. For instance, vector particle-in-call (VPIC) \cite{bowers20080, bowers2008ultrahigh, bowers2009advances} and Flecsale \cite{flecsale} are two Department of Energy (DOE) simulation codes that are designed specifically for in situ workflow with real-time visualization using ParaView, so that users can make real-time decisions based on the simulation progress.

However, it is difficult to manually control the process of launching and debugging large-scale workflows that consist of hundreds of tasks with different kinds of dependency modes. Many workflow management systems have been developed to ease this complication by automating the launching process. Among all workflow management systems \cite{liu2015survey, altintas2004kepler, deelman2005pegasus, ogasawara2013chiron, zhao2007swift}, to the best of our knowledge, none of them natively support launching workflows with in situ dependencies between tasks. To launch workflow with in situ analysis, current users need to manually launch each task and configure in situ dataflow between tasks. In most of the cases, tasks with in situ dependencies need to be launched in a time sensitive manner simultaneously, causing considerable difficulties to current users if some tasks require complicated launching processes. The manual approach also costs users significant time and effort while manually re-launching the whole or partial workflow for debugging or reproducing experiments.

In this paper, we propose an in situ analysis enabled workflow management system -- \texttt{BeeFlow}. Specifically, the main contributions of \texttt{BeeFlow} include:

\begin{itemize}
\item \textbf{In situ support:} \texttt{BeeFlow} supports in situ dependencies between tasks in addition to traditional offline dependencies. It allows users to define both in situ workflows \cite{sewell2015large} and offline workflows;

\item \textbf{In situ dataflow and control design:} \texttt{BeeFlow} is designed to support most kinds of dataflow between tasks in modern in situ workflows, including filesystem-based sharing and network-based sharing. It also supports backward simulation control from analysis tools;

\item \textbf{Container support:} \texttt{BeeFlow} is built based on the Build and Execution Environment (\texttt{BEE}) \cite{bee}, a Docker container supported universal execution framework. So, \texttt{BeeFlow} uses Docker images for user application development. This is especially beneficial when deploying large-scale workflows on production systems because it can save considerable time and effort for application configurations and minimizes compatibility issues on target systems. Also, BEE-atified HPC application containers ensure \texttt{BeeFlow} can run on most production HPC and cloud computing environments;
 
\item \textbf{Multiple platforms support:} Benefiting from the virtualized Docker environment, \texttt{BeeFlow} can be easily   configured to run on multiple platforms and easily switched between  HPC and cloud platforms (e.g., Amazon Web Services (AWS)) when needed.
\end{itemize}

We conduct evaluations that show \texttt{BeeFlow} has usability and performance similar to current scientific workflow management systems. We also showcase three scientific workflows using \texttt{BeeFlow}. The three workflows cover both in situ and offline workflows with two kinds of dataflow modes. 

The rest of this paper is organized as follows:  
we introduce backgrounds in section II and give formal dependency definitions in section III. In section IV, we discuss the design details of \texttt{BeeFlow}. Case study and evaluation are discussed in section V. In section VI, we discuss other large scale scientific workflow management tools and how \texttt{BeeFlow} differs. Finally, we conclude in section VII.




