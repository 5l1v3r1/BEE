\section{Introduction}
  \label{sec:introduction}

High Performance Computing (HPC) systems have become critical infrastructure for science and industry. For example, domain experts use HPC systems to run large-scale physical simulations, big data analysis, multi-layer artificial neural networks, molecular dynamics experiments, and DNA sequencing.

As different HPC systems typically have customized software environments, HPC users must often configure and build their application for each specific machine, which is time consuming and can become a bottleneck to productivity. Moreover, in some cases, years or even decades-old legacy applications still serve as key components in the process of scientific or industry research.  These legacy applications may have no active support and often require specific deprecated versions of libraries and/or hardware in order to make them run correctly. It may be hard to find or build library implementations that meet the requirement of legacy applications while remaining compatible with current HPC systems. These requirements impose great challenges for HPC users.  

In a shared resource computing environment, users commonly implement checkpoint/restoration to stop and resume their computations across allocations, because of time or resource limitations. Differences in software environments may impede the migration of checkpoints across systems that have different software/hardware configurations. A consistent execution environment is also important for HPC application developers. Consistency between developing, testing, and production environments can greatly save developers' time on fixing compatibility issues, which can significantly accelerate the development process.

Virtualized environments, and particularly virtual machines (VMs), have been thoroughly investigated for HPC systems to provide more consistent, isolated, and secure environments \cite{vallee2008system, reuther2012hpc}. For example, \cite{huang2006case} built a virtualized HPC environment using Xen-based VMs. By leveraging high performance I/O passthrough \cite{liu2006high}, VMs can achieve near-native performance when running HPC benchmarks. In \cite{zhang2016slurm}, it is shown that current resource managers in HPC systems cannot well supervise VMs and associated critical resources, so they proposed Slurm-V, which extends Slurm with virtualization-oriented capabilities. \cite{gugnani2016performance, tikotekar2008analysis} characterized the performance  of running multiple kinds of applications on virtualized HPC clusters. \cite{huang2007virtual} proposed Inter-VM Communication (IVC), a VM-aware communication library to support efficient shared memory communication among computing processes on the same physical host and then they built an MPI library that was IVC enabled.

However, managing application specific VM images is not trivial, given that VM images need to contain files of the operating system, dependent libraries/packages, user applications, and input/output data, which may take several gigabytes of disk space. Migrating images between HPC systems or distributing images among compute nodes can consume a lot of time. On the other hand, Linux container technologies like Docker \cite{Docker, awscontainer} are more lightweight and yet still provide consistent execution environments for development, build, and deployment. By using Docker, developers only need to build their application once in Docker on their local machine, and then the application can run on any Docker-enabled machine. For doing that, one only needs to pull the Docker images to the target machine. The Docker image only contains minimal operating system composition, application-dependent libraries/packages, and user application, with much less space requirement than VM images \cite{boettiger2015introduction}. Also, since Docker creates a thin translation layer that allows the guest application to share much of the host operating system, the performance penalty is negligible \cite{merkel2014docker, ruan2016performance}.

It would be of great benefit to bring the advantages of Docker, as realized in the cloud, to HPC users; however, Docker is not usually supported on current HPC systems. The main reason behind it is that Docker requires a privileged service and a Linux kernel version higher than 3.8, while Linux kernel versions on current HPC production systems are typically far behind this \cite{harji2013our}. Especially for national security oriented research facilities, such as Los Alamos National Laboratory, it can take years of security and compatibility evaluation before their production HPC systems can upgrade Linux kernel. 

Although HPC systems that run containers are being actively deployed using software such as Shifter \cite{jacobsen2015contain} and Singularity \cite{kurtzer_2016_60736}, BEE-VM's approach is complementary.  HPC systems that have been built and deployed with Shifter or Singularity can run containerized applications without a VM layer. However, containers must be built or adapted for Singularity and Shifter.  BEE-VM brings containerized applications to other HPC machines and, because it runs Docker natively, does not require a rebuild when moving applications across machines. 

In this work, we design a Docker-enabled execution framework on HPC with the following design goals:

\begin{enumerate}
\item \textbf{Automation:}
Both the deployment of container-enabled execution environment and the user's application should be done automatically. 
\item \textbf{Portability:}
The execution framework should run across multiple platforms, including HPC systems and cloud computing systems.
\item \textbf{Flexibility:}
Users should have the flexibility to choose different platforms as their needs change and also migrate application states (via checkpoints) to different platforms. 
\item \textbf{Reproducibility:}
Applications should behave the same across platforms regardless of the host software configurations.
\item \textbf{Interfacing Continuous Integration:}
The execution framework should support integration with commonly used Continuous Integration (CI) workflows in scientific application development, so that it can effectively free users from handling complicated application compilation.  
\end{enumerate}

%our work
In this work, we design a new build and execution environment that can enable the execution of Message Passing Interface (MPI) jobs in Docker containers on almost any current HPC system. We call it the Build and Execution Environment (\texttt{BEE}). We specifically detail our standard \texttt{BEE} backend that solves the Linux kernel version issue and provides a more flexible design environment, through a specialized VM containing native Docker, the \texttt{BEE-VM}.  For machines that support Kernel-based Virtual Machine (KVM) (on by default in Linux), a hardware accelerated type 1 KVM hypervisor provides bare metal performance.  For machines without KVM, we build and configure Quick Emulator (QEMU) in user space. QEMU runs on many host operating systems and on Linux, since kernel 2.6, which makes \texttt{BEE-VM} compatible with almost all HPC machines; however, QEMU without paravirtualization does introduce a significant performance penalty. By using BOTO API \cite{BOTOAPI}, we also provide the BEE backend solution on running MPI jobs in Docker container on cloud platforms. We have a \texttt{BEE-AWS} backend that is able to deploy \texttt{BEE} execution environment on Amazon Web Service (AWS) instances. We also have a \texttt{BEE-Chameleon}, which is able to deploy \texttt{BEE} execution environment on Chameleon Cloud \cite{chameleon}. Most importantly, our solution does not require root/administration privileges of the HPC system, so any user can deploy \texttt{BEE} on HPC systems. Because we use native Docker, users can always benefit from the features of the latest Docker release. The contributions of this work include:

\begin{enumerate}
\item \textbf{Docker-enabled environment on HPC systems:}
By using \texttt{BEE-VM}, we provide Docker-enabled environments into current HPC systems. HPC users can easily Dockerize their application to run on \texttt{BEE} without reconfiguration across different HPC systems. Users can also Dockerize their legacy applications and run on \texttt{BEE} on current HPC systems unmodified.

\item \textbf{User space deployment on unmodified HPC systems:}
On all QEMU compatible systems, users can deploy \texttt{BEE-VM} on the HPC system they are using without root.

\item \textbf{Standard latest Docker support:}
We run unmodified Docker daemon inside \texttt{BEE-VM}, so users can Dockerize their application in the standard way. Existing Dockerized application can run on \texttt{BEE-VM} unmodified. Also, Docker inside \texttt{BEE-VM} is upgradeable so that users can always benefit from the latest features of Docker.

\item \textbf{Interfacing CI:}
Many CI pipelines of current HPC application development projects are already using Docker, so users can run CI-generated Dockerized application on \texttt{BEE} with minimal modification.

\item \textbf{Both software and hardware virtualization:}
Existing solutions run containers directly on the host, however, different hardware architecture on different hosts may yield different result. With our solution, we provide a hardware virtualization layer, so that together with Docker container we can, if desired, provide a consistent hardware and software environment. This enhances the reproducibility of \texttt{BEE-VM}.


\item \textbf{Hybrid HPC and cloud environment:}
Besides HPC systems, \texttt{BEE} can also be deployed on cloud computing environments, such as AWS and Chameleon Cloud. We provide the same ability to execute Docker containers to users, so that users can run their HPC application on the cloud without modification.

\begin{comment}
\item \textbf{Workflow integration:}
The containerized environment provided by \texttt{BEE} can greatly facilitate the HPC workflow integration without complicated application configurations. Different parts in the workflow can be packed in container and viewed as modules, so we can easily integrate different modules together to form the workflow we want using \texttt{BEE}.
\end{comment}

%\item \textbf{Cross platform system level live migration:}
%s we mentioned before, the capability of application level checkpoint/restoration is limited. On the other hand, system level checkpoint/restoration is a better choice, since it does not bounded by the specific application characteristic. So, we integrate Docker level live migration feature in \texttt{BEE}. Users can choose to do live migrate their application across different HPC systems or cloud computing systems.

\end{enumerate}

The rest of this paper is organized as follows:  
we discuss the BEE framework design in section II and backend \texttt{BEE-VM} in section III. In section IV and V, we introduce the design of \texttt{BEE-AWS} and \texttt{BEE-Chameleon}. Performance is evaluated in section VI. We showcase VPIC, a real HPC application running on \texttt{BEE-VM} in section VII. In section VIII, we discuss other large scale container-enabled execution environments and how \texttt{BEE} is different. Finally, we conclude in section IX.





